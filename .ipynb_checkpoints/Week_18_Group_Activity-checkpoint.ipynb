{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79077b8e",
   "metadata": {},
   "source": [
    "# Week 18\n",
    "## Intro to Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f74898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dependencies and Modules:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# diabetes.csv file:\n",
    "diabetes_path = \"C:/Users/Nik/Documents/diabetes.csv\"\n",
    "diabetes_df = pd.read_csv(diabetes_path)\n",
    "diabetes_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770a0bf",
   "metadata": {},
   "source": [
    "## 1. Look up the Adam optimization functions in PyTorch https://pytorch.org/docs/stable/optim.html. \n",
    "How does it work? Try at least one other optimization function with the diabetes dataset shown in class. How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532912bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing my training and test sets:\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "sc=StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7e362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the sets into tensors for the optimizers:\n",
    "#convert to tensors\n",
    "X_train = torch.FloatTensor(X_train) \n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train) \n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2fae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class for the Advanced Neural Network module:\n",
    "class ANN_Model(nn.Module):\n",
    "    \n",
    "    #I checked documentation to choose best parameters:\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features=2):\n",
    "        \n",
    "        # I use the keyword 'super' to iolates changes and ensure\n",
    "        # children are calling the right parents:\n",
    "        super().__init__() \n",
    "        \n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply the functions:\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a38a25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer <class 'torch.optim.adadelta.Adadelta'>: Epoch number 500 with loss, 0.43025681376457214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       150\n",
      "           1       0.66      0.54      0.59        81\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.72      0.69      0.70       231\n",
      "weighted avg       0.73      0.74      0.73       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adagrad.Adagrad'>: Epoch number 500 with loss, 0.11430498212575912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       150\n",
      "           1       0.54      0.54      0.54        81\n",
      "\n",
      "    accuracy                           0.68       231\n",
      "   macro avg       0.65      0.65      0.65       231\n",
      "weighted avg       0.68      0.68      0.68       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adam.Adam'>: Epoch number 500 with loss, 0.043085530400276184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       150\n",
      "           1       0.60      0.57      0.58        81\n",
      "\n",
      "    accuracy                           0.71       231\n",
      "   macro avg       0.69      0.68      0.68       231\n",
      "weighted avg       0.71      0.71      0.71       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adamw.AdamW'>: Epoch number 500 with loss, 0.02749692089855671\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74       150\n",
      "           1       0.52      0.56      0.54        81\n",
      "\n",
      "    accuracy                           0.67       231\n",
      "   macro avg       0.64      0.64      0.64       231\n",
      "weighted avg       0.67      0.67      0.67       231\n",
      "\n",
      "Optimizer <class 'torch.optim.adamax.Adamax'>: Epoch number 500 with loss, 0.02106991410255432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74       150\n",
      "           1       0.53      0.57      0.55        81\n",
      "\n",
      "    accuracy                           0.67       231\n",
      "   macro avg       0.64      0.65      0.64       231\n",
      "weighted avg       0.68      0.67      0.67       231\n",
      "\n",
      "Optimizer <class 'torch.optim.asgd.ASGD'>: Epoch number 500 with loss, 0.021069293841719627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74       150\n",
      "           1       0.53      0.57      0.55        81\n",
      "\n",
      "    accuracy                           0.67       231\n",
      "   macro avg       0.64      0.65      0.64       231\n",
      "weighted avg       0.68      0.67      0.67       231\n",
      "\n",
      "Optimizer <class 'torch.optim.nadam.NAdam'>: Epoch number 500 with loss, 0.4551573395729065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.63      0.72       150\n",
      "           1       0.53      0.75      0.62        81\n",
      "\n",
      "    accuracy                           0.68       231\n",
      "   macro avg       0.68      0.69      0.67       231\n",
      "weighted avg       0.72      0.68      0.68       231\n",
      "\n",
      "Optimizer <class 'torch.optim.radam.RAdam'>: Epoch number 500 with loss, 0.1943582147359848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.65      0.72       150\n",
      "           1       0.52      0.72      0.60        81\n",
      "\n",
      "    accuracy                           0.67       231\n",
      "   macro avg       0.67      0.68      0.66       231\n",
      "weighted avg       0.71      0.67      0.68       231\n",
      "\n",
      "Optimizer <class 'torch.optim.rmsprop.RMSprop'>: Epoch number 500 with loss, 0.36432957649230957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.57      0.68       150\n",
      "           1       0.50      0.80      0.62        81\n",
      "\n",
      "    accuracy                           0.65       231\n",
      "   macro avg       0.67      0.69      0.65       231\n",
      "weighted avg       0.72      0.65      0.66       231\n",
      "\n",
      "Optimizer <class 'torch.optim.rprop.Rprop'>: Epoch number 500 with loss, 0.24778813123703003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.62      0.72       150\n",
      "           1       0.53      0.79      0.63        81\n",
      "\n",
      "    accuracy                           0.68       231\n",
      "   macro avg       0.69      0.71      0.67       231\n",
      "weighted avg       0.73      0.68      0.69       231\n",
      "\n",
      "Optimizer <class 'torch.optim.sgd.SGD'>: Epoch number 500 with loss, 0.2701990306377411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.59      0.70       150\n",
      "           1       0.52      0.81      0.63        81\n",
      "\n",
      "    accuracy                           0.67       231\n",
      "   macro avg       0.68      0.70      0.66       231\n",
      "weighted avg       0.74      0.67      0.67       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we go!\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model:\n",
    "ann = ANN_Model()\n",
    "\n",
    "# Define a loss function:\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a list of all the optimizers I want to check:\n",
    "optimizer_list = [torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.AdamW,\n",
    "                  torch.optim.Adamax, torch.optim.ASGD, torch.optim.NAdam, torch.optim.RAdam, \n",
    "                  torch.optim.RMSprop,torch.optim.Rprop, torch.optim.SGD]\n",
    "\n",
    "# Here is the for loop to test the different optimizers:\n",
    "for x in optimizer_list:\n",
    "    \n",
    "    optimizer = x(ann.parameters(), lr=0.1)\n",
    "    \n",
    "    #run model through multiple iterations (or epochs)\n",
    "    final_loss = []\n",
    "    n_epochs = 501\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = ann.forward(X_train)\n",
    "        loss = loss_function(y_pred, y_train)\n",
    "        final_loss.append(loss)\n",
    "        # Printing loss value at final epoch:\n",
    "        if epoch == 500:\n",
    "            print(f'Optimizer {x}: Epoch number {epoch} with loss, {loss}')\n",
    "\n",
    "        # Fire up each optimizer with .zero_grad which will zero the gradient before \n",
    "        # running backwards propagation for meaningful comparison:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 500:\n",
    "        # Make the predictions: \n",
    "            y_pred = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(X_test):\n",
    "                    prediction = ann(data)\n",
    "                    y_pred.append(prediction.argmax())\n",
    "                    \n",
    "# Print a report for each optimizer after final epoch:\n",
    "            print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e37535",
   "metadata": {},
   "source": [
    "### The report above is of the classification report and loss function determination of loss for every optimizer it made sense for us to try. The only optimizers that can be said to have outperformed Adam in some way are: AdaDelta, which had better statistical metrics. AdamW and Adamax, which had half the loss as Adam.\n",
    "\n",
    "### I think AdaDelta had better metrics on its report because (almost identical to AdaGrad) it uses a moving window of gradient updates and keeps learning longer than other optimizers. So it makes sense that it's model had better stats.\n",
    "\n",
    "### I think AdamW and Adamax had smaller loss than Adam because they have updated weight decay algorithms to improve on stock Adam. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a347a5",
   "metadata": {},
   "source": [
    "## 2. Write a function that lists and counts the number of divisors for an input value.\n",
    "**Example 1:**\\\n",
    "Input: 5\\\n",
    "Output: “There are 2 divisors: 1 and 5”\n",
    "\n",
    "**Example 2:**\\\n",
    "Input: 40\\\n",
    "Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb95d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 6 divisors in 12. They are: 1, 2, 3, 4, 6, and 12'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the separator variable\n",
    "separator = \", \"\n",
    "\n",
    "# Defining the function:\n",
    "def factor_fun(num):\n",
    "    factors = []\n",
    "    for i in np.arange(1, num+1):\n",
    "        if num % i == 0:\n",
    "            factors.append(i)\n",
    "    else:\n",
    "        pass\n",
    "    return f'There are {len(factors)} divisors in {num}. They are: {(separator.join(map(str, factors[:-1])))}, and {factors[-1]}'\n",
    "factor_fun(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da61a475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 2 divisors in 5. They are: 1, and 5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_fun(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 8 divisors in 40. They are: 1, 2, 4, 5, 8, 10, 20, and 40'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_fun(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5f4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
